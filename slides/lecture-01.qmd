---
title: "Demo: Data Science Education with WebAssembly"
subtitle: "Linear Regression in R and Python"
format: 
    live-revealjs: 
        scrollable: true
webr:
    packages: 
        - ggplot2
pyodide: 
    packages: 
        - scikit-learn
        - pandas
        - matplotlib
        - seaborn
        - statsmodels

engine: knitr
---

{{< include ../_extensions/r-wasm/live/_knitr.qmd >}}

## Overview

The goal of this presentation is to showcase the power of WebAssembly (WASM) in data science education by allowing real-time code execution, visualization, and exercises directly within the slide deck.

We do this by exploring the concept of linear regression using both R and Python code snippets.


---

## Introduction

Linear regression is a fundamental statistical technique used to model the relationship between a dependent variable and one or more independent variables.

This presentation will cover:

1. Basic Concepts
2. Implementation in R and Python
3. Model Evaluation
4. Assumptions and Diagnostics

---

## Basic Concepts

Linear regression aims to find the best-fitting straight line through the data points.

The general form of a simple linear regression model is:

$$Y = \beta_0 + \beta_1X + \epsilon$$

Where:

- $Y$ is the dependent variable
- $X$ is the independent variable
- $\beta_0$ is the y-intercept
- $\beta_1$ is the slope
- $\epsilon$ is the error term

---

## Generating Data

Let's look at how to implement linear regression in R and Python by first simulating some data


## R

```{webr}
# Create sample data
set.seed(123)
x <- 1:100
y <- 2 * x + 1 + rnorm(100, mean = 0, sd = 3)
df <- data.frame(x = x, y = y)

head(df)
```


---

## Guessing the Coefficients

Try to fit a linear regression model by hand through manipulating coefficients below:

The linear regression with $\beta_0 =$
`{ojs} beta_0_Tgl` and $\beta_1 =$ `{ojs} beta_1_Tgl` is:

```{ojs}
//| echo: false
import {Tangle} from "@mbostock/tangle"

// Setup Tangle reactive inputs
viewof beta_0 = Inputs.input(0);
viewof beta_1 = Inputs.input(1);
beta_0_Tgl = Inputs.bind(Tangle({min: -30, max: 300, minWidth: "1em", step: 1}), viewof beta_0);
beta_1_Tgl = Inputs.bind(Tangle({min: -5, max: 5, minWidth: "1em", step: 0.25}), viewof beta_1);

// draw plot in R
regression_plot(beta_0, beta_1)
```

```{webr}
#| edit: false
#| output: false
#| define:
#|   - regression_plot
regression_plot <- function(beta_0, beta_1) {

  # Create scatter plot  
  plot(
    df$x, df$y, 
    xlim = c(min(df$x) - 10, max(df$x) + 10),
    ylim = c(min(df$y) - 10, max(df$y) + 10)
  )

  # Graph regression line
  abline(a = beta_0, b = beta_1, col = "red")
}
```
:::


---

## Fit Linear Regression Model

Now that we have our data, let's fit a linear regression model to it:



## R

```{webr}
# Fit linear regression model
model <- lm(y ~ x, data = df)

# View summary of the model
summary(model)
```


## Visualize the Results

We can visualize the data and the regression line to see how well the model fits the data using ggplot2 in R and Matplotlib in Python.



## R

```{webr}
library(ggplot2) 

# Plot the data and regression line
ggplot(df, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  theme_minimal() +
  labs(title = "Linear Regression in R",
       x = "X", y = "Y")
```


---

## Predicting New Values

We can use our linear regression model to make predictions on new data:



## R

```{webr}
# Predict new values
new_data <- data.frame(x = c(101, 102, 103))
predictions <- predict(model, newdata = new_data)

predictions
```


---

## Your Turn: Predict New Values!

{{< countdown "01:30" top="10px" right="5px">}}

Create a new data frame with `x` values 10, 30, and 60, then use the model to predict the corresponding y values.


## R 

```{webr}
#| exercise: ex_1_r
# Create your new data frame here
_______

# Make predictions here
_______

# Print the predictions
_______
```

```{webr}
#| exercise: ex_1_r
#| check: true

# Create your new data frame here
new_data <- data.frame(x = c(10, 30, 60))

# Make predictions here
predictions <- predict(model, newdata = new_data)

if (isTRUE(all.equal(.result, predictions))) {
  list(correct = TRUE, message = "Nice work!")
} else {
  list(correct = FALSE, message = "That's incorrect, sorry.")
}
```

---

## Model Evaluation

We can evaluate the performance of our linear regression model using various metrics:



## R

```{webr}
# R-squared
summary(model)$r.squared

# Root Mean Squared Error (RMSE)
sqrt(mean(residuals(model)^2))

# Mean Absolute Error (MAE)
mean(abs(residuals(model)))
```


---

## Assumptions 

Linear regression relies on several assumptions:

1. Linearity
2. Independence
3. Homoscedasticity
4. Normality of residuals

---

## Checking Assumptions with Diagnostics Plots

Let's look at some diagnostic plots:



## R

```{webr}
par(mfrow = c(2, 2))
plot(model)
```

---

## Conclusion

- Linear regression is a powerful tool for modeling relationships between variables.
- Both R and Python offer robust implementations and diagnostic tools.
- Always check assumptions and perform diagnostics to ensure the validity of your model.
- Consider more advanced techniques (e.g., multiple regression, polynomial regression) for complex relationships.